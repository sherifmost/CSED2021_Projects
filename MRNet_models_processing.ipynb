{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRNet_models_processing.ipynb",
      "provenance": [],
      "mount_file_id": "1CfZr__riY1RwLWtPx4FhIhmCHGfX1rVn",
      "authorship_tag": "ABX9TyODqqc0snKB2Tf8gV5DUmD5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherifmost/CSED2021_Projects/blob/master/MRNet_models_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjwQJbS4DNac",
        "colab_type": "text"
      },
      "source": [
        "## **All needed library imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQvR4fkNDRyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51JL8xSfFsI-",
        "colab_type": "text"
      },
      "source": [
        "# **Importing the needed notebooks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df7_CoUgFxuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f96004f-e219-4448-c328-b25b2831e8e9"
      },
      "source": [
        "%cd '/content/drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkXvMPfPFyqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "767c7dce-0512-4a0f-f6ce-22334347fb4b"
      },
      "source": [
        "!pip install import-ipynb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=96c281431bc3bd4ad2ab673c6504e77e0cbe85e95fdc5832ea61e01959d2782b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CSf0OtnF3Te",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "af38d7eb-11eb-47ce-f155-5f42ff2b39da"
      },
      "source": [
        "import import_ipynb\n",
        "# importing the needed notebooks\n",
        "import MRnet_data_extraction as data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from MRnet_data_extraction.ipynb\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "unzip:  cannot find or open /content/drive/My Drive/MRNET data set/MRNet-v1.0.zip, /content/drive/My Drive/MRNET data set/MRNet-v1.0.zip.zip or /content/drive/My Drive/MRNET data set/MRNet-v1.0.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK4uMa5xI90O",
        "colab_type": "text"
      },
      "source": [
        "# **Training functions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67C5BfqJJCgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_extractor(model:Model,model_type,series,anomaly):\n",
        "  # obtaining the data\n",
        "  x,y = data.get_data_train(series,anomaly);\n",
        "  input_train,output_train,input_validate,output_validate = data.split(x,y);\n",
        "  # processing to train for the extractor specifically: obtaining only one image from each patient\n",
        "  input_train = get_one_image(input_train);\n",
        "  input_validate = get_one_image(input_validate);\n",
        "  # normalizing the input data\n",
        "  input_train = data.normalize(input_train);\n",
        "  # defining the call backs for training\n",
        "  save_path = data.path_model + data.delim + model_type + data.delim + data.extractor + data.delim+series + '_' + anomaly + data.extension_model;\n",
        "  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n",
        "  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n",
        "  # performing a traininig operation with a batch size to overcome any overfitting\n",
        "  training_history = model.fit(x = input_train, y = output_train,  validation_data=(input_validate,output_validate), batch_size = 20, epochs=50, callbacks=[save_best,stop]);\n",
        "  # plotting the graph for the training history\n",
        "  plot(training_history);\n",
        "  return training_history;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwhjnY5nN2hH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_one_image(input_train):\n",
        "  new_input_train = [];\n",
        "  for curr in input_train:\n",
        "     new_input_train.append(curr[0]);\n",
        "  return new_input_train;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEHXjJJcPfUX",
        "colab_type": "text"
      },
      "source": [
        "# **Functions for model evaluation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Fx0T_TPjMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(history):\n",
        "  # drawing the model graph\n",
        "  pd.DataFrame(history.history).plot(figsize=(10, 7));\n",
        "  plt.grid(True);\n",
        "  plt.gca().set_ylim(0, 1);\n",
        "  plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHWzlCbpQZja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_extractor(model:Model,series,anomaly):\n",
        "  # getting test data\n",
        "  input_test,output_test = data.get_data_test(series,anomaly);\n",
        "  # processing it to test the extractor specifically: getting one image from each patient only\n",
        "  input_test = get_one_image(input_test);\n",
        "  return model.evaluate(input_test,output_test);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuHJEl2sR6oF",
        "colab_type": "text"
      },
      "source": [
        "# **Functions to manipualte saved models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLK3NnZ1R-eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model_type,series,anomaly,model_part):\n",
        "  model_path = data.path_model + data.delim + model_type + data.delim + model_part + data.delim+series + '_' + anomaly + data.extension_model;\n",
        "  return tf.keras.models.load_model(model_path);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qkf3HzuSkjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}